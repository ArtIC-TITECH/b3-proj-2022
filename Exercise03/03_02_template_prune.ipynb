{
 "cells": [
  {
   "source": [
    "This tutorial is based on PyTorch's tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html and contains the code snippets from it:  \n",
    "- device test\n",
    "- transform, trainset, trainloader, testset, testloader, classes\n",
    "\n",
    "The license of the original tutorial is the 3-Clause BSD License.  \n",
    "See LICENSE for detail.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/b3_proj_2022/MyModules')\n",
    "import util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# To monitor the server's GPU installation and usage: log in the server and run `nvidia-smi`.\n",
    "# It shows the list of GPUs online and their utilization.\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size_train = 128\n",
    "batch_size_test = 128\n",
    "num_shown_images = 8\n",
    "input_size = 32\n",
    "# input_size = 64\n",
    "\n",
    "study_name = \"exercise03_02_st01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    torchvision.transforms.Resize(input_size),\n",
    "    transforms.RandomCrop(input_size, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.424, 0.415, 0.384), (0.283, 0.278, 0.284))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    torchvision.transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.424, 0.415, 0.384), (0.283, 0.278, 0.284))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/b3_proj_2022/data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/b3_proj_2022/data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Conv層定義用ユーティリティ関数\n",
    "def conv_block(ich, och, ksize, num_layers, *, bn=True, pool=False, act=None, **kwargs):\n",
    "    assert num_layers >= 1\n",
    "    r = OrderedDict()\n",
    "    for i in range(num_layers):\n",
    "        r[str(i)] = nn.Conv2d(ich, och, ksize, **kwargs)\n",
    "        if bn:\n",
    "            r[\"%d-bn\" % i] = nn.BatchNorm2d(och)\n",
    "        ich = och  # set #input_channels to current #output_channels for next loop\n",
    "    if pool:\n",
    "        r[\"pool\"] = nn.MaxPool2d(2, 2)\n",
    "    if act is not None:\n",
    "        r[\"act\"] = act\n",
    "    return nn.Sequential(r)\n",
    "\n",
    "\n",
    "# FC層定義用ユーティリティ関数\n",
    "def fc_block(ich, och, *, bn=True, pool=False, act=None, **kwargs):\n",
    "    r = OrderedDict()\n",
    "    r[\"1\"] = nn.Linear(ich, och, **kwargs)\n",
    "    if bn:\n",
    "        r[\"1-bn\"] = nn.BatchNorm1d(och)\n",
    "    if act is not None:\n",
    "        r[\"act\"] = act\n",
    "    return nn.Sequential(r)\n",
    "\n",
    "# ネット定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, trial, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Trialからハイパーパラメータをsuggestしてもらう\n",
    "        num_conv_blocks = 4\n",
    "        num_conv_layers_per_block = 2\n",
    "        num_fc_layers = 2\n",
    "        \n",
    "        # suggest関数群:\n",
    "        # suggest_uniform(\"name\", min, max)\n",
    "        # suggest_loguniform(\"name\", min, max)\n",
    "        # suggest_categorical(\"name\", [item1, item2, ...])\n",
    "        # suggest_int(\"name\", min, max)\n",
    "\n",
    "        if num_fc_layers > 1:\n",
    "            fc_hidden_size = 1024\n",
    "        act = nn.ReLU()\n",
    "        \n",
    "        # Conv層を実体化\n",
    "        conv_blocks = []\n",
    "        conv_blocks.append(\n",
    "            (\"conv1\", conv_block(3, 64, 3, 1, bn=True, pool=True, act=act, padding=1, bias=False))\n",
    "        )\n",
    "        conv_blocks.append(\n",
    "            (\"conv2\", conv_block(64, 128, 3, 1, bn=True, pool=True, act=act, padding=1, bias=False))\n",
    "        )\n",
    "        \n",
    "        # Conv層の実体化 … 上でもらったパラメータを使う\n",
    "        ich = 128\n",
    "        och_max = 512\n",
    "        for i in range(3, num_conv_blocks + 1):\n",
    "            och = min(ich * 2, och_max)\n",
    "            conv_blocks.append(\n",
    "                (\"conv%d\" % i, conv_block(ich, och, 3, num_conv_layers_per_block, bn=True, pool=True, act=act, padding=1, bias=False))\n",
    "            )\n",
    "            ich = och\n",
    "        \n",
    "        # FC層の実体化\n",
    "        fc_blocks = []\n",
    "        ich = och * (input_size >> num_conv_blocks) ** 2  # och is still in the scope after the previous FOR statement! 気持ち悪い!\n",
    "        self.fc_input_size = ich\n",
    "        i = 1\n",
    "        for _ in range(1, num_fc_layers):\n",
    "            fc_blocks.append(\n",
    "                (\"fc%d\" % i, fc_block(ich, fc_hidden_size, bn=True, act=act, bias=False))\n",
    "            )\n",
    "            i += 1\n",
    "            ich = fc_hidden_size\n",
    "        fc_blocks.append(\n",
    "            (\"fc%d\" % i, fc_block(ich, num_classes, bn=False, act=None, bias=False))  # 最終層だけactとochの扱いが違う\n",
    "        )\n",
    "        \n",
    "        # モデル定義 … Sequentialの利用\n",
    "        self.conv_blocks = nn.Sequential(OrderedDict(conv_blocks))\n",
    "        self.fc_blocks = nn.Sequential(OrderedDict(fc_blocks))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(-1, self.fc_input_size)\n",
    "        x = self.fc_blocks(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "# 学習ルーチン\n",
    "# これを後でループする\n",
    "def train(net, trial):\n",
    "    date_str = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    basename = \"%s-%s\" % (study_name, date_str)\n",
    "    \n",
    "    print(\"Starting training for '%s'\" % basename)\n",
    "    \n",
    "    # Learning rate and momentum are also hyper-parameters!\n",
    "    optim_type = 'adam'  #trial.suggest_....?\n",
    "    if optim_type == 'adam':\n",
    "        lr = 1e-3  # trial.suggest_........(\"lr_adam\")\n",
    "        # suggest結果での条件分岐もOK\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr = 1e-3  # trial.suggest_........(\"lr_sgd\")\n",
    "        momentum=0.9\n",
    "        optimizer = optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.1)\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _ = dataiter.next()\n",
    "    writer = SummaryWriter(\"/content/drive/My Drive/Colab Notebooks/b3_proj_2022/runs/%s\" % basename)\n",
    "    writer.add_graph(net, images.to(device))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Save initial state\n",
    "    util.add_param(writer, net, 0)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            net.train()\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 99:\n",
    "                    train_acc = util.accuracy_batch(outputs, labels)\n",
    "                    print('[%d, %5d] loss: %.3f, train batch acc: %2d %%' %\n",
    "                          (epoch + 1, i + 1, running_loss, train_acc))\n",
    "\n",
    "                    gstep = epoch * len(trainloader) + i\n",
    "                    writer.add_scalar('Training/Loss', running_loss, gstep)\n",
    "                    writer.add_scalar('Training/Accuracy', train_acc, gstep)\n",
    "\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            # Evaluate intermediate result\n",
    "            gstep = epoch * len(trainloader) + i\n",
    "            net.eval()\n",
    "            with util.IntermediateOutputWriter(writer, net, gstep):\n",
    "                test_acc = util.accuracy(testloader, net, device=device)\n",
    "                print('[%d,      ] test acc: %2d %%' %\n",
    "                      (epoch + 1, test_acc))\n",
    "            writer.add_scalar('Test/Accuracy', test_acc, gstep)\n",
    "            util.add_param(writer, net, gstep)\n",
    "\n",
    "            # 早期打ち止めの判定\n",
    "            trial.report(test_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                print(\"Trial is being pruned.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "    finally:\n",
    "        # 打ち止めの場合でも保存はする\n",
    "        print('Finished Training')\n",
    "\n",
    "        dirpath = 'saved_models'\n",
    "        PATH = '%s/%s.pth' % (dirpath, basename)\n",
    "        try:\n",
    "            os.mkdir(dirpath)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        trial.set_user_attr(\"saved_path\", PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trialを引数にとる目的関数\n",
    "# Studyにより１施行当たり1回、自動的にループされる\n",
    "# Trialからパラメータをsuggestしてもらって実行、結果をreturnする\n",
    "def objective(trial):\n",
    "    net = Net(trial, num_classes)\n",
    "    net.to(device)\n",
    "    \n",
    "    try:\n",
    "        train(net, trial)\n",
    "    finally:\n",
    "        net.eval()\n",
    "        acc = util.accuracy(testloader, net, device=device)\n",
    "        print(\"Train finished.\\n%.1f %% at %s.\" % (acc, trial.params))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "current_path = '/content/drive/My Drive/Colab Notebooks/b3_proj_2022/'\n",
    "db_name ='exercise03_01_st01.db'\n",
    "con = sqlite3.connect(current_path+db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Prunerの作成\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=3, interval_steps=1)\n",
    "# 途中精度が中央値以下だったら捨てる\n",
    "# 3 trials揃うまでは判断しない。また各trialで3エポック数えるまで判断しない\n",
    "\n",
    "# Studyの作成\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "        # Studyを区別する一意の名前。同じ名前のStudyは同じパラメータ空間に入る\n",
    "    storage='sqlite:///'+current_path+db_name,\n",
    "    load_if_exists=True,\n",
    "        # 同じ名前のStudyがDB上に存在する場合それを読み込み、ない場合新規作成する\n",
    "        # Falseで同じ名前が存在する時はエラーで止まる\n",
    "        #\n",
    "        # | load_if_exists ->  | True               | False              |\n",
    "        # | ------------------ | ------------------ | ------------------ |\n",
    "        # | Study is in DB     | Reuse it           | Error              |\n",
    "        # | Study is not in DB | Create a new study | Create a new study |\n",
    "    direction='maximize',\n",
    "        # 最適化は最大化する方向\n",
    "    pruner=pruner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化を回す\n",
    "study.optimize(objective, n_trials=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適な成果を取得\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# その時のパラメータと学習済み係数でモデルを復元\n",
    "net = Net(optuna.trial.FixedTrial(best_trial.params))\n",
    "net.load_state_dict(torch.load(best_trial.user_attrs['saved_path']))\n",
    "net.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images[:num_shown_images]\n",
    "labels = labels[:num_shown_images]\n",
    "\n",
    "# print images\n",
    "util.imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(num_shown_images)))\n",
    "\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(num_shown_images)))\n",
    "accuracy_per_class, accuracy = util.accuracy_of_classes(num_classes, testloader, net)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % accuracy)\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], accuracy_per_class[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の出力\n",
    "# CSVにするのも簡単なので試してみよう\n",
    "\n",
    "keys = [\"##\", \"#start_date\", \"#state\", \"#acc\", \"saved_path\"]\n",
    "header = [\"#\", \"Start Date\", \"State\", \"Acc.\", \"Saved in\"]\n",
    "keys_set = set(keys)\n",
    "\n",
    "l = []\n",
    "for trial in study.trials:\n",
    "    row = {\"##\":trial.number, \"#start_date\":trial.datetime_start, \"#state\":trial.state, \"#acc\":trial.value}\n",
    "    row.update(trial.params)\n",
    "    row.update(trial.user_attrs)\n",
    "    l.append(row)\n",
    "    keys_set.update(row.keys())\n",
    "\n",
    "keys_rem = list(keys_set - set(keys))\n",
    "keys.extend(keys_rem)\n",
    "header.extend(keys_rem)\n",
    "res = [[record.get(k, None) for k in keys] for record in l]\n",
    "res.insert(0, header)\n",
    "util.show_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
